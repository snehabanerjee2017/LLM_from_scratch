{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "616854c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gpt_download import download_and_load_gpt2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f90d6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'data/the-verdict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "349d9598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw text: 20479 characters\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "print(f\"Length of raw text: {len(raw_text)} characters\")\n",
    "print(raw_text[:100])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913080e",
   "metadata": {},
   "source": [
    "#### Raw text to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "78514bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4670"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.;?_!\"()\\']|--|\\s)', raw_text) # split on commas, periods, question amrks, quotation marks, double dashes and whitespace\n",
    "preprocessed = [item for item in preprocessed if item.strip()] # remove whitespace-only tokens\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6be4d1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius',\n",
       " '--',\n",
       " 'though',\n",
       " 'a',\n",
       " 'good',\n",
       " 'fellow',\n",
       " 'enough',\n",
       " '--',\n",
       " 'so',\n",
       " 'it',\n",
       " 'was',\n",
       " 'no',\n",
       " 'great',\n",
       " 'surprise',\n",
       " 'to',\n",
       " 'me',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'that',\n",
       " ',',\n",
       " 'in']"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437de9a9",
   "metadata": {},
   "source": [
    "#### Token to Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3843ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1146 tokens\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocab size: {vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "5fc79847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindle:', 43)\n",
      "('Grindles', 44)\n",
      "('HAD', 45)\n",
      "('Had', 46)\n",
      "('Hang', 47)\n",
      "('Has', 48)\n",
      "('He', 49)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i < 50:\n",
    "        print(item)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54f087",
   "metadata": {},
   "source": [
    "#### Text tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "5f5f8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer: token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        token_ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        text = ' '.join([self.int_to_str[token_id] for token_id in token_ids])\n",
    "        text = re.sub(r'\\s+([,.;?_!\"()\\'])', r'\\1', text) # remove space before punctuation\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "06317e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d236b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 57, 2, 861, 1002, 607, 536, 755, 5, 1142, 601, 5, 1, 68, 7, 38, 862]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a5dc9962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80c74a",
   "metadata": {},
   "source": [
    "#### Tokenizer handles unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "fd4539c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "907c9841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "b14dd823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1143)\n",
      "('your', 1144)\n",
      "('yourself', 1145)\n",
      "('<|endoftext|>', 1146)\n",
      "('<|unk|>', 1147)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "d227a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer: token for token, integer in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        token_ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        text = ' '.join([self.int_to_str[token_id] for token_id in token_ids])\n",
    "        text = re.sub(r'\\s+([,.;?_!\"()\\'])', r'\\1', text) # remove space before punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ea4a277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "72e02bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "c7bc7936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1147, 5, 357, 1142, 634, 989, 10, 1146, 56, 1002, 970, 998, 730, 1002, 1147, 7]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "6584b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d83faa",
   "metadata": {},
   "source": [
    "#### Byte Pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4205b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "11475a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "5c799afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ba4b3",
   "metadata": {},
   "source": [
    "#### Data sampling with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "d8310151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokenized text: 5145 tokens\n"
     ]
    }
   ],
   "source": [
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"Length of tokenized text: {len(enc_text)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "17667b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "c095a9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence (x): [290, 4920, 2241, 287]\n",
      "Target sequence (y):      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "print(\"Input sequence (x):\", x)\n",
    "print(\"Target sequence (y):     \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "6d0c4677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -> 4920\n",
      "[290, 4920] -> 2241\n",
      "[290, 4920, 2241] -> 287\n",
      "[290, 4920, 2241, 287] -> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "31ca5889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ->  established\n",
      " and established ->  himself\n",
      " and established himself ->  in\n",
      " and established himself in ->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    \n",
    "\n",
    "    print(tokenizer.decode(context), \"->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0aea3",
   "metadata": {},
   "source": [
    "#### Dataset for batched inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f4663f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "0e633ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(text, batch_size = 4, max_length = 256, stride = 128, shuffle = True, drop_last = True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e859e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False, drop_last=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "0774ec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "b134ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False, drop_last=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60169068",
   "metadata": {},
   "source": [
    "#### Token embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "53ed997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "469a681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "ed7675af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "95553067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b2e9c",
   "metadata": {},
   "source": [
    "#### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8b45b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 256\n",
    "vocab_size = 50257  # GPT-2 vocabulary size\n",
    "token_embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "bfe68c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Inputs shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "batch_size = 8\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=batch_size, max_length=max_length, stride=max_length, shuffle=False, drop_last=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs: \\n\", inputs)\n",
    "print(\"Inputs shape:\", inputs.shape)  # Expected: (batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b94a577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Token Embeddings shape:\", token_embeddings.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "569bfc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Embeddings shape: torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(num_embeddings=context_length, embedding_dim=output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional Embeddings shape:\", pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "ff3b4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Input Embeddings shape:\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca112ad2",
   "metadata": {},
   "source": [
    "#### Self attention without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "805242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with \n",
    "     [0.77, 0.25, 0.10], # one\n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "cfdc372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query, x_i)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "fcb1dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention Weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a74adc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (naive softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention Weights (naive softmax):\", attn_weights_2_naive)\n",
    "print(\"Sum: \", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2512f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (torch.softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention Weights (torch.softmax):\", attn_weights_2)\n",
    "print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "f2b7b32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context Vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "06b00846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores Matrix:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i, x_j)\n",
    "\n",
    "# for loops are slower, matric multiplication does the same thing more efficiently\n",
    "print(\"Attention Scores Matrix:\\n\", attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8064aa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores Matrix (using @ operator):\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(\"Attention Scores Matrix (using @ operator):\\n\", attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "1f60912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights Matrix:\n",
      " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(\"Attention Weights Matrix:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "46f45a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "edbe586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Context Vectors:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(\"All Context Vectors:\\n\", all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "686e2b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Context Vectors:\n",
      " tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous Context Vectors:\\n\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561697ad",
   "metadata": {},
   "source": [
    "#### Self attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "a21c613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "d2ffb0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False) # requires_grad=False since we are not training\n",
    "W_key = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "efaf3b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vector:\n",
      " tensor([-1.1729, -0.0048])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(\"Query Vector:\\n\", query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8f3ea9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys shape:\n",
      " torch.Size([6, 2])\n",
      "Values shape:\n",
      " torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"Keys shape:\\n\", keys.shape)\n",
    "print(\"Values shape:\\n\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ffeb333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores (Query 2 vs Key 2):\n",
      " tensor(0.1376)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "print(\"Attention Scores (Query 2 vs Key 2):\\n\", attn_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "baf1233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores (Query 2 vs All Keys):\n",
      " tensor([ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(\"Attention Scores (Query 2 vs All Keys):\\n\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "1f6c0956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (Query 2 vs All Keys):\n",
      " tensor([0.1704, 0.1611, 0.1652, 0.1412, 0.2505, 0.1117])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(\"Attention Weights (Query 2 vs All Keys):\\n\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "4d34e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector (Query 2):\n",
      " tensor([0.2854, 0.4081])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context Vector (Query 2):\\n\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "7631dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV1(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(SelfAttentionV1, self).__init__()\n",
    "        self.W_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_key = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_value = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        queries = inputs @ self.W_query\n",
    "        keys = inputs @ self.W_key\n",
    "        values = inputs @ self.W_value\n",
    "\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "\n",
    "        context_vectors = attn_weights @ values\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "74e9788c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2845, 0.4071],\n",
      "        [0.2854, 0.4081],\n",
      "        [0.2854, 0.4075],\n",
      "        [0.2864, 0.3974],\n",
      "        [0.2863, 0.3910],\n",
      "        [0.2860, 0.4039]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e438218",
   "metadata": {},
   "source": [
    "#### Self attention using Linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "0be7658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super(SelfAttentionV2, self).__init__()\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1], dim=-1)\n",
    "\n",
    "        context_vectors = attn_weights @ values\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "9904b421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0748,  0.0699],\n",
      "        [-0.0755,  0.0693],\n",
      "        [-0.0755,  0.0692],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0766,  0.0675],\n",
      "        [-0.0759,  0.0685]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d96073",
   "metadata": {},
   "source": [
    "#### Causal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "5a298f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(\"Attention weights:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "69b8f262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask:\n",
      " tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones((context_length, context_length)))\n",
    "print(\"Causal Mask:\\n\", mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "3b2817f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Attention Weights:\n",
      " tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(\"Masked Attention Weights:\\n\", masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "201ea235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Masked Attention Weights:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(\"Normalized Masked Attention Weights:\\n\", masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "3a2ad8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Attention Scores:\n",
      " tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones((context_length, context_length)), diagonal=1).bool()\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Masked Attention Scores:\\n\", masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "7f960b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Attention Weights:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(\"Masked Attention Weights:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "d766f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vectors with Causal Masking:\n",
      " tensor([[ 0.1196, -0.3566],\n",
      "        [ 0.2501,  0.0845],\n",
      "        [ 0.2996,  0.2571],\n",
      "        [ 0.2913,  0.3103],\n",
      "        [ 0.2869,  0.3839],\n",
      "        [ 0.2830,  0.3669]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_vec = attn_weights @ values\n",
    "print(\"Context Vectors with Causal Masking:\\n\", context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "76141455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(p=0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e5d25035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6e058",
   "metadata": {},
   "source": [
    "#### Casual Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d1ffb1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack([inputs, inputs], dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "60c366b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, qkv_bias=False, dropout=0.1):\n",
    "        super(CasualAttention, self).__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vectors = attn_weights @ values\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "d24ac647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vecs shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length=context_length, dropout=0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"Context vecs shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19dc179",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "62fc38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout, qkv_bias=False):\n",
    "        super(MultiHeadAttentionWrapper, self).__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            CasualAttention(d_in, d_out, context_length, qkv_bias, dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "0e2acc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vecs shape: torch.Size([2, 6, 4])\n",
      "Context vecs: tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, num_heads=2, context_length=context_length, dropout=0.0)\n",
    "context_vecs = mha(batch)\n",
    "print(\"Context vecs shape:\", context_vecs.shape)\n",
    "print(\"Context vecs:\", context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3ecd9",
   "metadata": {},
   "source": [
    "#### Efficient Multi-head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "842334cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dropout, qkv_bias=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_out % num_heads == 0\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.out_proj = torch.nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1,2).contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "285031a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vecs shape: torch.Size([2, 6, 2])\n",
      "Context vecs: tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length=context_length, num_heads=2, dropout=0.0)\n",
    "context_vecs = mha(batch)\n",
    "print(\"Context vecs shape:\", context_vecs.shape)\n",
    "print(\"Context vecs:\", context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233ae3d",
   "metadata": {},
   "source": [
    "#### GPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "bf2f35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"n_layers\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"emb_dim\": 768,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f538c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DummyTransformerBlock, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "3d9e3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyGPTModel, self).__init__()\n",
    "        self.tok_emb = torch.nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = torch.nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = torch.nn.Dropout(p=cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = torch.nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = torch.nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = torch.nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len).unsqueeze(0).repeat(batch_size,1))\n",
    "        x = self.drop_emb(tok_embeds + pos_embeds)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "51ef4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyLayerNorm(torch.nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(DummyLayerNorm, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "0b81310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "eb952cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "Output: tensor([[[-0.7867,  0.2203, -0.4508,  ..., -0.9936, -0.1412, -0.2999],\n",
      "         [-0.0788,  0.3004, -0.2935,  ...,  0.1583,  0.8917,  0.8230],\n",
      "         [ 0.3708,  1.1126, -0.3226,  ...,  0.8023, -0.0038,  0.3935],\n",
      "         [ 0.0636,  1.0572, -0.2507,  ...,  0.7542, -0.0750, -0.6896]],\n",
      "\n",
      "        [[-0.7208,  0.1351, -0.6014,  ..., -1.0272,  0.1729, -0.2920],\n",
      "         [-0.5938,  0.4453, -0.0059,  ...,  0.3414,  0.0572,  1.0986],\n",
      "         [ 0.2675,  0.8407, -0.4476,  ..., -0.0181, -0.1090,  0.2541],\n",
      "         [-0.1035, -0.5901, -0.3932,  ...,  1.4022, -0.3188,  0.1304]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(\"Output:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "7f816843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = torch.nn.Sequential(torch.nn.Linear(5,6), torch.nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "9ff556d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "var = out.var(dim=-1,keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "50586438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Output:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean after normalization:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance after normalization:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1,keepdim=True)\n",
    "var = out_norm.var(dim=-1,keepdim=True)\n",
    "print(\"Normalized Output:\\n\", out_norm)\n",
    "print(\"Mean after normalization:\\n\", mean)\n",
    "print(\"Variance after normalization:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "ad225f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "7a11e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * x_norm + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "6abd4d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean after LayerNorm:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance after LayerNorm:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1,keepdim=True)\n",
    "var = out_ln.var(dim=-1,keepdim=True, unbiased=False)\n",
    "print(\"Mean after LayerNorm:\\n\", mean)\n",
    "print(\"Variance after LayerNorm:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "8f644ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GELU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "24c81a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gelu, relu = GELU(), torch.nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, steps=100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8,3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"])):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.plot(x,y, label=label)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "5cf06e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            torch.nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "20c73b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.randn(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "adbb46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super(ExampleDeepNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(torch.nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            torch.nn.Sequential(torch.nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            torch.nn.Sequential(torch.nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),  \n",
    "            torch.nn.Sequential(torch.nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            torch.nn.Sequential(torch.nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "0ed5fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "8cfea961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = torch.nn.MSELoss()(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "fd18bd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "0e8d3664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "891fbea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = torch.nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = torch.nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = torch.nn.Dropout(p=cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "c3f30801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.randn(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "479f60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(GPTModel, self).__init__()\n",
    "        self.tok_emb = torch.nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = torch.nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = torch.nn.Dropout(p=cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = torch.nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = torch.nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = torch.nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len).unsqueeze(0).repeat(batch_size,1))\n",
    "        x = self.drop_emb(tok_embeds + pos_embeds)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "b2fc784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "Output:\n",
      " tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(\"Output:\\n\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "0e6fe02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the model: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in the model: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "0440be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Token embeddinng layer shape: torch.Size([50257, 768])\n",
      " Output head layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\" Token embeddinng layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\" Output head layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "85f339f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in GPT-2 with weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.tok_emb.parameters())\n",
    "print(f\"Total number of parameters in GPT-2 with weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "a6b54988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model size (float32): 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 ** 2)\n",
    "print(f\"Total model size (float32): {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "af19809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "ec3ddf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: [15496, 11, 314, 716]\n",
      "Encoded input tensor shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"Encoded input:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) \n",
    "print(\"Encoded input tensor shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "40dbcc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 6,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "3b89b2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(\"Generated text:\\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "45c3c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"n_layers\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"emb_dim\": 768,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "9342cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "e3ed80f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    return tokenizer.decode(token_ids.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "922640f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "b0d53691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Generated text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da1741",
   "metadata": {},
   "source": [
    "#### Text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "e8cd33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # [every effort moves]\n",
    "                       [40, 1107, 588]]) #[\"I really like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "93b71c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345], # [effort moves you]\n",
    "                        [588, 428, 11311]])  # [really like chocolate]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "6a61f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probas shape: torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(\"Probas shape:\", probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5d5f26b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token IDs:\n",
      " tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Predicted token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "44545529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Gathering SerbianFriday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(start_dim=0), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "12d9b57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target probabilities batch 1: tensor([2.3466e-05, 2.0531e-05, 1.1733e-05])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=True)\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Target probabilities batch 1:\", target_probas_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "08b9a8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target probabilities batch 1: tensor([1.3380e-05, 1.3445e-05, 1.1586e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Target probabilities batch 1:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "0db6f04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities of target tokens: tensor([-10.6600, -10.7936, -11.3531, -11.2217, -11.2169, -11.3658])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "log_probas = torch.log(torch.cat([target_probas_1, target_probas_2], dim=0))\n",
    "print(\"Log probabilities of target tokens:\", log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "9a80e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average log probability of target tokens: tensor(-11.1018)\n"
     ]
    }
   ],
   "source": [
    "avg_log__probas = torch.mean(log_probas)\n",
    "print(\"Average log probability of target tokens:\", avg_log__probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "b231bef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative average log probability (Cross-Entropy Loss): tensor(11.1018)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log__probas * -1\n",
    "print(\"Negative average log probability (Cross-Entropy Loss):\", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646fe688",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "68f6e6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([2, 3, 50257])\n",
      "targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "c6eb0b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits shape: torch.Size([6, 50257])\n",
      "Flattened targets shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(start_dim=0, end_dim=1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits shape:\", logits_flat.shape)\n",
    "print(\"Flattened targets shape:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "8be6a728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: tensor(11.1018)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\"Cross-Entropy Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762f722",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "d47919b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: tensor(66292.8359)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908bd716",
   "metadata": {},
   "source": [
    "#### Training with variable lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "9b63fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "ac704ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in text: 20479\n",
      "Total tokens in text: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(f\"Total characters in text: {total_characters}\")\n",
    "print(f\"Total tokens in text: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "76ae831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(len(text_data) * train_ratio)\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "86ff28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "dc11b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n",
      "Validation loader:\n",
      "Input batch shape: torch.Size([2, 256])\n",
      "Target batch shape: torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(\"Input batch shape:\", x.shape)\n",
    "    print(\"Target batch shape:\", y.shape)\n",
    "\n",
    "print(\"Validation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(\"Input batch shape:\", x.shape)\n",
    "    print(\"Target batch shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "429c46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(start_dim=0, end_dim=1),\n",
    "        target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "0670aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "98ef4a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train Loss: 10.9885\n",
      "Validation Loss: 10.9903\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(f\"Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "36af1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "0d6858ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model = model,\n",
    "            idx = encoded,\n",
    "            max_new_tokens = 50,\n",
    "            context_size = context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "a3ab3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                model.eval()\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1}, Step {global_step:06d}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        generate_and_print_sample(model, train_loader.dataset.tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "a63f3782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 000000: Train Loss = 9.8964, Val Loss = 9.9327\n",
      "Epoch 1, Step 000005: Train Loss = 7.9162, Val Loss = 8.3380\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Epoch 2, Step 000010: Train Loss = 6.8458, Val Loss = 7.0472\n",
      "Epoch 2, Step 000015: Train Loss = 5.9962, Val Loss = 6.6160\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Epoch 3, Step 000020: Train Loss = 5.6618, Val Loss = 6.6073\n",
      "Epoch 3, Step 000025: Train Loss = 5.3515, Val Loss = 6.3413\n",
      "Every effort moves you, and I had been.                                            \n",
      "Epoch 4, Step 000030: Train Loss = 4.0013, Val Loss = 6.2778\n",
      "Epoch 4, Step 000035: Train Loss = 2.5330, Val Loss = 6.2245\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Epoch 5, Step 000040: Train Loss = 3.8244, Val Loss = 6.1617\n",
      "Every effort moves you know it was not that the picture--I had the fact with the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Epoch 6, Step 000045: Train Loss = 2.5056, Val Loss = 6.1765\n",
      "Epoch 6, Step 000050: Train Loss = 2.9911, Val Loss = 6.1445\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a and he was no I had the fact, and in the picture.                    \n",
      "Epoch 7, Step 000055: Train Loss = 2.5567, Val Loss = 6.1361\n",
      "Epoch 7, Step 000060: Train Loss = 1.7366, Val Loss = 6.2380\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Epoch 8, Step 000065: Train Loss = 0.9426, Val Loss = 6.2411\n",
      "Epoch 8, Step 000070: Train Loss = 1.4724, Val Loss = 6.2448\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Epoch 9, Step 000075: Train Loss = 0.7792, Val Loss = 6.2955\n",
      "Epoch 9, Step 000080: Train Loss = 0.6212, Val Loss = 6.4002\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, my eye fell on a small picture\n",
      "Epoch 10, Step 000085: Train Loss = 0.3896, Val Loss = 6.4598\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.01)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=1,\n",
    "    start_context=\"Every effort moves you\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "013e06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label='Train Loss')\n",
    "    ax1.plot(epochs_seen, val_losses, label='Validation Loss', linestyle='-.')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel('Tokens Seen')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "961f27f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, steps=len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59027a18",
   "metadata": {},
   "source": [
    "#### Decoding strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f507fd7",
   "metadata": {},
   "source": [
    "##### Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "c504a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "6b27b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
    "probas = torch.softmax(next_token_logits, dim=-1)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "6927f5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "31814cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 X closer\n",
      "0 X every\n",
      "0 X effort\n",
      "582 X forward\n",
      "2 X inches\n",
      "0 X moves\n",
      "0 X pizza\n",
      "343 X toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} X {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba7c57",
   "metadata": {},
   "source": [
    "##### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "d6eae867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "c7f09c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, temp) for temp in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], width=bar_width, label=f\"Temperature = {T}\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ea2f8",
   "metadata": {},
   "source": [
    "##### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "373f2307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "tok_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, tok_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "6146c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New logits after top-k filtering: tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(\"New logits after top-k filtering:\", new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "56203825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k probabilities: tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=-1)\n",
    "print(\"Top-k probabilities:\", topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e31c43",
   "metadata": {},
   "source": [
    "#### Text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "69661494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            logits = torch.where(\n",
    "                condition=logits < top_logits[:, -1],\n",
    "                input=torch.tensor(float('-inf')).to(logits.device),\n",
    "                other=logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "0853ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens = 15,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"],\n",
    "    temperature = 1.4,\n",
    "    top_k = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "481bdd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text with temperature and top-k sampling:\n",
      " Every effort moves youlit terrace.\n",
      "\n",
      "\n",
      "\n",
      "\" he said deprecating laugh\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated text with temperature and top-k sampling:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817c276",
   "metadata": {},
   "source": [
    "#### Loading and saving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "71d93597",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "e673f300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fb5c5",
   "metadata": {},
   "source": [
    "##### Save both model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "d2907807",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }, \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "f2612078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.01)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505279f",
   "metadata": {},
   "source": [
    "#### Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "0660e180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x11967afd0>)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch05/\"\n",
    "       \"01_main-chapter-code/gpt_download.py\")\n",
    "\n",
    "filename = url.split(\"/\")[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "a886c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|| 77.0/77.0 [00:00<00:00, 66.8kiB/s]\n",
      "encoder.json: 100%|| 1.04M/1.04M [00:01<00:00, 983kiB/s] \n",
      "hparams.json: 100%|| 90.0/90.0 [00:00<00:00, 132kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|| 498M/498M [03:08<00:00, 2.65MiB/s]  \n",
      "model.ckpt.index: 100%|| 5.21k/5.21k [00:00<00:00, 2.66MiB/s]\n",
      "model.ckpt.meta: 100%|| 471k/471k [00:00<00:00, 787kiB/s] \n",
      "vocab.bpe: 100%|| 456k/456k [00:00<00:00, 754kiB/s] \n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "057b8980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "57b9aeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor shape: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor shape:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "1c46c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12, \"context_length\": 1024, \"qkv_bias\": True},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16, \"context_length\": 1024, \"qkv_bias\": True},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20, \"context_length\": 1024, \"qkv_bias\": True},\n",
    "    \"gpt2-xl (1.5B)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25,  \"context_length\": 1024, \"qkv_bias\": True},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "400d2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "5a45ff5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'n_layers': 12,\n",
       " 'n_heads': 12,\n",
       " 'emb_dim': 768,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': True}"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "51590a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "3a1eb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "a63fe588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T\n",
    "        )\n",
    "\n",
    "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=0)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        ) \n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.weight = assign(\n",
    "            gpt.trf_blocks[b].norm1.weight,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm1.bias = assign(\n",
    "            gpt.trf_blocks[b].norm1.bias,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].norm2.weight = assign(\n",
    "            gpt.trf_blocks[b].norm2.weight,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.bias = assign(\n",
    "            gpt.trf_blocks[b].norm2.bias,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "    gpt.final_norm.weight = assign(\n",
    "        gpt.final_norm.weight,\n",
    "        params[\"g\"]\n",
    "    )\n",
    "    gpt.final_norm.bias = assign(\n",
    "        gpt.final_norm.bias,\n",
    "        params[\"b\"]\n",
    "    )\n",
    "    gpt.out_head.weight = assign(\n",
    "        gpt.out_head.weight,\n",
    "        params[\"wte\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "285abad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "fa03403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text with GPT-2 weights:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model = gpt,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens = 25,\n",
    "    context_size = NEW_CONFIG[\"context_length\"],\n",
    "    top_k = 50,\n",
    "    temperature = 1.5\n",
    ")\n",
    "print(\"Generated text with GPT-2 weights:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a65467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
